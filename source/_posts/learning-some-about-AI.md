---
title: AI学习
abbrlink: 8944de35
date: 2024-12-28 20:44:51
updated: 2025-02-14 09:40:37
tags:
  - AI
categories: 人工智能
mathjax: true
---

# 前言
这部分属于了解学习的，很多数学概念~~难~~

<!-- more -->

# 机器学习


## 一些算法


以下是关于 **机器学习经典算法** 的高频面试题总结，涵盖核心原理、数学推导、优缺点及实际应用场景：

---

### **一、基础概念**
1. **监督学习 vs 无监督学习的区别？**  
   - **监督学习**：使用带标签的数据训练模型（如分类、回归），目标是最小化预测值与真实值的误差。  
   - **无监督学习**：使用无标签的数据发现隐藏模式（如聚类、降维）。  

2. **什么是过拟合？如何解决？**  
   - **过拟合**：模型在训练集表现好，但在测试集表现差（高方差）。  
   - **解决方法**：增加数据量、正则化（L1/L2）、降低模型复杂度、交叉验证、早停（Early Stopping）。  

3. **偏差（Bias）与方差（Variance）的权衡？**  
   - **高偏差**：模型欠拟合（如线性模型拟合非线性数据）。  
   - **高方差**：模型过拟合（如复杂树模型）。  
   - **优化方向**：调整模型复杂度、正则化、集成方法。  

---

### **二、经典监督学习算法**
4. **线性回归**  
   - **目标函数**：最小化均方误差（MSE）:  
     $$ J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 $$  
   - **闭式解**：$\theta = (X^T X)^{-1} X^T y$（若 $X^T X$ 可逆）。  
   - **正则化**：  
     - **Ridge 回归（L2）**：$\theta = (X^T X + \lambda I)^{-1} X^T y$。  
     - **Lasso 回归（L1）**：通过梯度下降求解，产生稀疏权重。  

5. **逻辑回归**  
   - **核心思想**：用 Sigmoid 函数将线性输出映射到概率：  
     $$ P(y=1|x) = \frac{1}{1 + e^{-\theta^T x}} $$  
   - **损失函数**：交叉熵损失（Log Loss）:  
     $$ J(\theta) = -\frac{1}{m} \sum_{i=1}^m \left[ y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log (1 - h_\theta(x^{(i)})) \right] $$  
   - **为什么用交叉熵而非 MSE？**  
     MSE 非凸，存在多个局部最优；交叉熵损失对概率敏感，梯度更稳定。  

6. **支持向量机（SVM）**  
   - **目标**：寻找最大间隔超平面，数学形式为：  
     $$ \min_{\theta} \frac{1}{2} \|\theta\|^2 \quad \text{s.t.} \quad y^{(i)}(\theta^T x^{(i)} + b) \geq 1 $$  
   - **核技巧（Kernel Trick）**：将数据映射到高维空间，解决线性不可分问题（如 RBF 核）。  
   - **软间隔 SVM**：引入松弛变量 $\xi$ 允许部分样本违反约束，平衡间隔与分类错误。  

7. **决策树**  
   - **划分标准**：  
     - **信息增益**（ID3 算法）：$ \text{Gain}(D, A) = H(D) - H(D|A) $。  
     - **信息增益比**（C4.5 算法）：解决信息增益对多值属性的偏好。  
     - **基尼指数**（CART 算法）：$ \text{Gini}(D) = 1 - \sum_{k=1}^K p_k^2 $。  
   - **剪枝策略**：预剪枝（提前终止分裂）和后剪枝（生成树后剪枝）。  

8. **随机森林**  
   - **核心思想**：Bagging（Bootstrap Aggregating） + 决策树，通过投票/平均降低方差。  
   - **特征随机性**：每棵树随机选择部分特征分裂，增强多样性。  

9. **梯度提升树（GBDT）与 XGBoost**  
   - **GBDT**：串行训练多棵决策树，每棵树拟合前一棵树的负梯度（残差）。  
   - **XGBoost 改进**：  
     - 目标函数加入正则化项（控制模型复杂度）。  
     - 使用二阶泰勒展开近似损失函数，支持并行化。  
   - **LightGBM vs XGBoost**：LightGBM 基于直方图优化，内存更低，训练更快。  

---

### **三、无监督学习算法**
10. **K-means 聚类**  
    - **步骤**：随机初始化簇中心 → 分配样本到最近簇 → 更新簇中心 → 迭代至收敛。  
    - **目标函数**：最小化样本到簇中心的距离平方和：  
      $$ J = \sum_{i=1}^k \sum_{x \in C_i} \|x - \mu_i\|^2 $$  
    - **缺点**：对初始值敏感，需手动指定 K 值，仅适用于凸数据集。  

11. **主成分分析（PCA）**  
    - **目标**：将数据投影到低维空间，保留最大方差。  
    - **数学推导**：协方差矩阵的特征值分解，取前 $k$ 大特征值对应的特征向量。  
    - **应用场景**：降维可视化、去噪、特征提取。  

---

### **四、模型评估与优化**
12. **分类任务常用评估指标**  
    - **准确率（Accuracy）**：正确样本比例，不适用于类别不平衡数据。  
    - **精确率（Precision）**：$ \frac{TP}{TP + FP} $（关注预测为正的准确性）。  
    - **召回率（Recall）**：$ \frac{TP}{TP + FN} $（关注真实正例的覆盖率）。  
    - **F1 Score**：$ 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} $。  
    - **AUC-ROC**：ROC 曲线下面积，衡量分类器整体性能。  

13. **梯度下降法（Gradient Descent）**  
    - **批量梯度下降（BGD）**：每次迭代使用全部样本，计算精确梯度，但速度慢。  
    - **随机梯度下降（SGD）**：每次迭代随机选一个样本，速度快但波动大。  
    - **小批量梯度下降（Mini-batch GD）**：折中方案，常用批量大小 32-256。  

---

### **五、高频进阶问题**
14. **逻辑回归为什么用极大似然估计而非最小二乘？**  
    - 最小二乘假设误差服从高斯分布，而逻辑回归输出概率更适合用伯努利分布的似然函数。  

15. **SVM 对偶问题的意义？**  
    - 对偶问题将原始优化问题转化为关于拉格朗日乘子的优化，便于引入核函数处理非线性问题。  

16. **如何解决类别不平衡问题？**  
    - 过采样（SMOTE）、欠采样、调整类别权重（如 class_weight）、使用 F1 Score 或 AUC 评估。  

17. **XGBoost 如何实现并行化？**  
    - 特征级别的并行：在分裂节点时，并行计算每个特征的增益。  

---

### **六、代码与数学题**
18. **手写逻辑回归梯度下降代码（伪代码）**  
    ```python
    def logistic_regression(X, y, lr=0.01, epochs=1000):
        theta = np.zeros(X.shape[1])
        for _ in range(epochs):
            z = np.dot(X, theta)
            h = 1 / (1 + np.exp(-z))
            gradient = np.dot(X.T, (h - y)) / len(y)
            theta -= lr * gradient
        return theta
    ```

19. **计算 K-means 的簇中心更新公式**  
    $$ \mu_j^{(new)} = \frac{1}{|C_j|} \sum_{x \in C_j} x $$  

20. **推导 SVM 的间隔公式**  
    - 超平面为 $\theta^T x + b = 0$，样本 $x_i$ 到超平面的距离为：  
      $$ \frac{|\theta^T x_i + b|}{\|\theta\|} $$  
    - 最大化最小间隔 $\Rightarrow$ 最小化 $\|\theta\|$。  



# 大模型推理
